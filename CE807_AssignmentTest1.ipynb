{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CE807_AssignmentTest1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTWOGoCnKFv3NOilnsckMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ah20776/CE807---Assignment/blob/main/CE807_AssignmentTest1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUw1OSDWMM0J",
        "outputId": "9c0b6bbb-6b65-4f91-b1bc-a6557f5edeb2"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "#try\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Importing libraries\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, nltk, spacy, gensim\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "'''\n",
        "Loading Gensim and nltk libraries\n",
        "'''\n",
        "# pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(400)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYnqu3bXMwTR",
        "outputId": "d0125b1b-bb0f-4a84-a527-97332110ebfb"
      },
      "source": [
        "# Load news data set\n",
        "# remove meta data headers footers and quotes from news dataset\n",
        "dataset = fetch_20newsgroups(shuffle=True,\n",
        "                            random_state=32,\n",
        "                            remove=('headers', 'footers', 'qutes'))\n",
        "\n",
        "#\n",
        "dataset_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=32, remove=('headers', 'footers', 'qutes'))\n",
        "dataset_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=32, remove=('headers', 'footers', 'qutes'))\n",
        "# Check the names of the categories\n",
        "print(dataset.target_names)\n",
        "print(dataset_train.target_names)\n",
        "print(dataset_test.target_names)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIsecRiMM9TR",
        "outputId": "a6ae48e4-51b0-4c88-c1ed-a69e1dac34e9"
      },
      "source": [
        "# put your data into a dataframe\n",
        "news_df_train = pd.DataFrame({'News': dataset_train.data,\n",
        "                       'Target': dataset_train.target})\n",
        "news_df_test = pd.DataFrame({'News': dataset_test.data,\n",
        "                       'Target': dataset_test.target})\n",
        "\n",
        "# get dimensions of data \n",
        "print(news_df_train.shape)\n",
        "print(news_df_train.head())\n",
        "\n",
        "print(news_df_test.shape)\n",
        "print(news_df_test.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 2)\n",
            "                                                News  Target\n",
            "0  The real question here in my opinion is what M...       4\n",
            "1  Please could someone in the US give me the cur...       4\n",
            "2  Can somebody please help me with information a...      12\n",
            "3  In article <2077@rwing.UUCP> pat@rwing.UUCP (P...      16\n",
            "4  From article <1pq6i2$a1f@news.ysu.edu>, by ak2...       7\n",
            "(7532, 2)\n",
            "                                                News  Target\n",
            "0  Radio Shack stores sell them here in the State...      12\n",
            "1  Has anyone else observed this behaviour and if...       2\n",
            "2  D. Andrew Byler (db7n+@andrew.cmu.edu) wrote:\\...      15\n",
            "3  Again I assume this is not just flame bait by ...      10\n",
            "4  In article <1993Apr21.100149.1501@rtsg.mot.com...       7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehwYTYcNNACN",
        "outputId": "ca0d828e-9c33-46be-8c17-c86ff6e4a2a9"
      },
      "source": [
        "#clean text data\n",
        "# remove non alphabetic characters\n",
        "# remove stopwords and lemmatize\n",
        "\n",
        "def clean_text(sentence):\n",
        "    # remove non alphabetic sequences\n",
        "    pattern = re.compile(r'[^a-z]+')\n",
        "    sentence = sentence.lower()\n",
        "    sentence = pattern.sub(' ', sentence).strip()\n",
        "    \n",
        "    # Tokenize\n",
        "    word_list = word_tokenize(sentence)\n",
        "    \n",
        "    # stop words\n",
        "    stopwords_list = set(stopwords.words('english'))\n",
        "    # puctuation\n",
        "    #punct = set(string.punctuation)\n",
        "    \n",
        "    # remove stop words\n",
        "    word_list = [word for word in word_list if word not in stopwords_list]\n",
        "    # remove very small words, length < 3\n",
        "    # they don't contribute any useful information\n",
        "    word_list = [word for word in word_list if len(word) > 2]\n",
        "\n",
        "    # remove punctuation\n",
        "    #word_list = [word for word in word_list if word not in punct]\n",
        "    \n",
        "    # stemming\n",
        "    # ps  = PorterStemmer()\n",
        "    # word_list = [ps.stem(word) for word in word_list]\n",
        "    \n",
        "    # lemmatize\n",
        "    lemma = WordNetLemmatizer()\n",
        "    word_list = [lemma.lemmatize(word) for word in word_list]\n",
        "    # list to sentence\n",
        "    sentence = ' '.join(word_list)\n",
        "    \n",
        "    return sentence\n",
        "\n",
        "# we'll use tqdm to monitor progress of data cleaning process\n",
        "# create tqdm for pandas\n",
        "tqdm.pandas()\n",
        "# clean text data\n",
        "\n",
        "news_df_train['News'] = news_df_train['News'].progress_apply(lambda x: clean_text(str(x)))\n",
        "print(news_df_train.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "100%|██████████| 11314/11314 [00:18<00:00, 596.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                News  Target\n",
            "0  real question opinion motorola processor runni...       4\n",
            "1  please could someone give current street price...       4\n",
            "2  somebody please help information american magn...      12\n",
            "3  article rwing uucp pat rwing uucp pat myrto wr...      16\n",
            "4  article news ysu edu yfn ysu edu john daker cu...       7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AVmO1OPPlQ0"
      },
      "source": [
        "news_NB = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
        "\n",
        "news_NB = news_NB.fit(news_df_train.News, news_df_train.Target)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwIJLtNJYxJp",
        "outputId": "85a279f3-0248-4397-d146-c1cf3fe3e451"
      },
      "source": [
        "# Performance of NB Classifier\n",
        "predicted_news_NB = news_NB.predict(news_df_test.News)\n",
        "np.mean(predicted_news_NB == news_df_test.Target)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7446893255443441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-8jbfKFZJv-",
        "outputId": "010738c3-39c4-4ce6-ca68-714e6dc277f7"
      },
      "source": [
        "# Training Support Vector Machines - SVM and calculating its performance\n",
        "\n",
        "news_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
        "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))])\n",
        "\n",
        "news_svm = news_svm.fit(news_df_train.News, news_df_train.Target)\n",
        "predicted_news_svm = news_svm.predict(news_df_test.News)\n",
        "np.mean(predicted_news_svm == news_df_test.Target)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7547796070100903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us-SmMQ0aWK3",
        "outputId": "ea278aee-646d-462e-ee14-e0981a134ba7"
      },
      "source": [
        "# Grid Search\n",
        "# Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
        "# All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
        "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
        "\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3, 1e-4)}\n",
        "\n",
        "# Next, we create an instance of the grid search by passing the classifier, parameters\n",
        "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
        "\n",
        "# with Naive Bayes\n",
        "\n",
        "gs_news_NB = GridSearchCV(news_NB, parameters, n_jobs=-1)\n",
        "gs_news_NB = gs_news_NB.fit(news_df_train.News, news_df_train.Target)\n",
        "\n",
        "# To see the best mean score and the params, run the following code\n",
        "\n",
        "print(gs_news_NB.best_score_)\n",
        "print(gs_news_NB.best_params_)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.893142011203175\n",
            "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jx6DvbAeMfg",
        "outputId": "bf1d9ca4-3347-4fee-e328-60f204b36e4a"
      },
      "source": [
        "#trying with svm\n",
        "\n",
        "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}\n",
        "\n",
        "gs_news_svm = GridSearchCV(news_svm, parameters_svm, n_jobs=-1)\n",
        "gs_news_svm = gs_news_svm.fit(news_df_train.News, news_df_train.Target)\n",
        "\n",
        "\n",
        "print(gs_news_svm.best_score_)\n",
        "print(gs_news_svm.best_params_)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8622066121159481\n",
            "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}