{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CE807_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhROfieecv1uVJJFwrYop7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ah20776/CE807---Assignment/blob/main/Assignment2/CE807_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7d51qaZQBHN",
        "outputId": "3ba780f7-1363-4c5b-c336-763ffac5ef85"
      },
      "source": [
        "!pip install pyLDAvis\n",
        "!pip install gensim\n",
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.2.4)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.20.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (54.2.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.20.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.20.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfKmTfErQFmj",
        "outputId": "d2be1873-4627-450a-a041-da2772f31f7e"
      },
      "source": [
        "# Load news data set\n",
        "# remove meta data headers footers and quotes from news dataset\n",
        "\n",
        "from pprint import pprint\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True,\n",
        "                            random_state=32,\n",
        "                            remove=('headers', 'footers', 'qutes'))\n",
        "\n",
        "#\n",
        "dataset_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=32, remove=('headers', 'footers', 'qutes'))\n",
        "dataset_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=32, remove=('headers', 'footers', 'qutes'))\n",
        "# Check the names of the categories\n",
        "pprint(dataset.target_names)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism',\n",
            " 'comp.graphics',\n",
            " 'comp.os.ms-windows.misc',\n",
            " 'comp.sys.ibm.pc.hardware',\n",
            " 'comp.sys.mac.hardware',\n",
            " 'comp.windows.x',\n",
            " 'misc.forsale',\n",
            " 'rec.autos',\n",
            " 'rec.motorcycles',\n",
            " 'rec.sport.baseball',\n",
            " 'rec.sport.hockey',\n",
            " 'sci.crypt',\n",
            " 'sci.electronics',\n",
            " 'sci.med',\n",
            " 'sci.space',\n",
            " 'soc.religion.christian',\n",
            " 'talk.politics.guns',\n",
            " 'talk.politics.mideast',\n",
            " 'talk.politics.misc',\n",
            " 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnY6QvfEQIOV",
        "outputId": "f5968c55-2355-43dd-f9b1-8d8346474c2c"
      },
      "source": [
        "\n",
        "'''\n",
        "Loading Gensim and nltk libraries\n",
        "'''\n",
        "# pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(400)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "from spacy.lang.en.examples import sentences"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLLTGpKcQLCZ"
      },
      "source": [
        "'''\n",
        "Write a function to perform the pre processing steps on the entire dataset\n",
        "'''\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return result"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifEGKJiBQNke",
        "outputId": "5e5ab17e-80ae-488c-c7d6-a0b8ac8a0267"
      },
      "source": [
        "processed_docs = []\n",
        "\n",
        "for doc in dataset_train.data:\n",
        "    processed_docs.append(preprocess(doc))\n",
        "\n",
        "'''\n",
        "Preview 'processed_docs'\n",
        "'''\n",
        "pprint(processed_docs[:2])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['real',\n",
            "  'question',\n",
            "  'opinion',\n",
            "  'motorola',\n",
            "  'processor',\n",
            "  'run',\n",
            "  'compar',\n",
            "  'intel',\n",
            "  'processor',\n",
            "  'run',\n",
            "  'window',\n",
            "  'recal',\n",
            "  'convers',\n",
            "  'run',\n",
            "  'window',\n",
            "  'benchmark',\n",
            "  'speed',\n",
            "  'know',\n",
            "  'true',\n",
            "  'love',\n",
            "  'hear',\n",
            "  'technic',\n",
            "  'data',\n",
            "  'david'],\n",
            " ['current',\n",
            "  'street',\n",
            "  'price',\n",
            "  'follow',\n",
            "  'relev',\n",
            "  'tax',\n",
            "  'simm',\n",
            "  'simm',\n",
            "  'refund',\n",
            "  'possibl',\n",
            "  'export',\n",
            "  'recommend',\n",
            "  'reliabl',\n",
            "  'supplier']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YutYF9dQPf4",
        "outputId": "4b96b661-9dad-4909-942b-2a1c0d165e79"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "bigram = gensim.models.Phrases(processed_docs, min_count=5, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[processed_docs], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "def remove_stopwords(texts):\n",
        "   return [[word for word in simple_preprocess(str(doc)) \n",
        "   if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "   return [bigram_mod[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "   [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "   texts_out = []\n",
        "   for sent in texts:\n",
        "      doc = nlp(\" \".join(sent))\n",
        "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "   return texts_out\n",
        "data_words_nostops = remove_stopwords(processed_docs)\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=[\n",
        "   'NOUN', 'ADJ', 'VERB', 'ADV'\n",
        "])\n",
        "print(data_lemmatized[:4]) #it will print the lemmatized data."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['real', 'question', 'opinion', 'window', 'recal', 'conver', 'run', 'window', 'benchmark', 'speed', 'know', 'true', 'love', 'hear', 'technic'], ['current', 'street', 'price', 'follow', 'refund', 'reliabl', 'supplier'], ['help', 'inform', 'card', 'reader', 'recent', 'buy', 'local', 'surplus', 'dealer', 'rear', 'follow', 'inform', 'card', 'reader', 'connector', 'power', 'connector'], ['write', 'sick', 'call', 'legisl', 'unseal', 'involv', 'atroc', 'includ', 'presid', 'attorney_general', 'governor', 'suspend', 'pend', 'serious', 'doubt']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYQWf1z0QSh1",
        "outputId": "5f1a9fa7-4c2d-4deb-ddb7-18423d28ea7a"
      },
      "source": [
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "print(corpus[:4]) #it will print the corpus we created above.\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]] \n",
        "#it will print the words with their frequencies.\n",
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "   corpus=corpus, id2word=id2word, num_topics=10, random_state=100, \n",
        "   update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2)], [(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)], [(15, 1), (21, 1), (22, 2), (23, 2), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1)], [(33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgp0x_iTQU_0",
        "outputId": "2d738952-16b2-4295-f755-8d66143e3d17"
      },
      "source": [
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]\n",
        "\n",
        "coherence_model_lda = CoherenceModel(\n",
        "   model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v'\n",
        ")\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.053*\"window\" + 0.045*\"card\" + 0.031*\"driver\" + 0.031*\"problem\" + '\n",
            "  '0.030*\"color\" + 0.023*\"version\" + 0.020*\"email\" + 0.020*\"monitor\" + '\n",
            "  '0.019*\"support\" + 0.019*\"run\"'),\n",
            " (1,\n",
            "  '0.030*\"peopl\" + 0.021*\"right\" + 0.020*\"say\" + 0.016*\"person\" + 0.016*\"true\" '\n",
            "  '+ 0.014*\"fact\" + 0.013*\"reason\" + 0.012*\"argument\" + 0.011*\"live\" + '\n",
            "  '0.011*\"claim\"'),\n",
            " (2,\n",
            "  '0.029*\"sell\" + 0.021*\"model\" + 0.020*\"pain\" + 0.018*\"car\" + 0.014*\"mile\" + '\n",
            "  '0.014*\"price\" + 0.013*\"metal\" + 0.013*\"finger\" + 0.013*\"food\" + '\n",
            "  '0.011*\"clean\"'),\n",
            " (3,\n",
            "  '0.036*\"write\" + 0.032*\"know\" + 0.023*\"think\" + 0.020*\"time\" + 0.016*\"go\" + '\n",
            "  '0.015*\"good\" + 0.014*\"look\" + 0.014*\"come\" + 0.014*\"want\" + 0.012*\"work\"'),\n",
            " (4,\n",
            "  '0.034*\"govern\" + 0.025*\"kill\" + 0.021*\"exist\" + 0.019*\"object\" + '\n",
            "  '0.016*\"deal\" + 0.015*\"attack\" + 0.014*\"moral\" + 0.013*\"armenian\" + '\n",
            "  '0.013*\"peopl\" + 0.011*\"state\"'),\n",
            " (5,\n",
            "  '0.053*\"game\" + 0.030*\"team\" + 0.030*\"play\" + 0.026*\"year\" + 0.025*\"player\" '\n",
            "  '+ 0.016*\"season\" + 0.016*\"point\" + 0.015*\"night\" + 0.014*\"panel\" + '\n",
            "  '0.014*\"lose\"'),\n",
            " (6,\n",
            "  '0.022*\"event\" + 0.019*\"report\" + 0.017*\"univer\" + 0.014*\"nation\" + '\n",
            "  '0.014*\"year\" + 0.012*\"research\" + 0.012*\"suffer\" + 0.012*\"blood\" + '\n",
            "  '0.011*\"doctor\" + 0.011*\"treatment\"'),\n",
            " (7,\n",
            "  '0.019*\"high\" + 0.018*\"list\" + 0.017*\"send\" + 0.016*\"mail\" + 0.015*\"number\" '\n",
            "  '+ 0.013*\"inform\" + 0.013*\"current\" + 0.012*\"develop\" + 0.011*\"cost\" + '\n",
            "  '0.011*\"space\"'),\n",
            " (8,\n",
            "  '0.034*\"output\" + 0.026*\"input\" + 0.023*\"manual\" + 0.020*\"print\" + '\n",
            "  '0.020*\"site\" + 0.017*\"recommend\" + 0.017*\"shape\" + 0.017*\"printer\" + '\n",
            "  '0.017*\"page\" + 0.016*\"flight\"'),\n",
            " (9,\n",
            "  '0.045*\"drive\" + 0.045*\"file\" + 0.037*\"wire\" + 0.036*\"program\" + '\n",
            "  '0.024*\"control\" + 0.017*\"disk\" + 0.017*\"type\" + 0.017*\"speed\" + '\n",
            "  '0.012*\"display\" + 0.012*\"chip\"')]\n",
            "\n",
            "Coherence Score:  0.4487095169430484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDklpYUcLu9R"
      },
      "source": [
        "#to find the best hyperparameters\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        "# Topics range\n",
        "min_topics = 2\n",
        "max_topics = 32\n",
        "step_size = 2\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "    \n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "                    \n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
        "    files.download('lda_tuning_results.csv')\n",
        "    pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EJKcYF0_QXL-",
        "outputId": "1e0ca38e-45d6-4b65-b796-07ce5398d81c"
      },
      "source": [
        "#finding the best number of topics with best hyperparameters\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "#With best parameters\n",
        "\n",
        "# Topics range\n",
        "min_topics = 5\n",
        "max_topics = 105\n",
        "step_size = 5\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "\n",
        "model_results = {'Topics': [],'Coherence': []}\n",
        "\n",
        "\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=len(topics_range))\n",
        "    for k in topics_range:\n",
        "        lda_model = gensim.models.ldamodel.LdaModel(\n",
        "        corpus=corpus, id2word=id2word, num_topics=k, random_state=100, \n",
        "        update_every=1, chunksize=100, passes=10, alpha=0.01, eta=0.61, per_word_topics=True\n",
        "        )\n",
        "\n",
        "    #print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
        "\n",
        "        coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "        coherence_lda = coherence_model_lda.get_coherence()\n",
        "        model_results['Topics'].append(k)\n",
        "        model_results['Coherence'].append(coherence_lda)\n",
        "        #print('\\nCoherence Score: ', coherence_lda)\n",
        "        pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('lda_tuning_results_new.csv', index=False)\n",
        "    files.download('lda_tuning_results_new.csv')\n",
        "    pbar.close()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [1:29:05<00:00, 413.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0d671a8e-db6c-4c5e-a3b5-6d0c7cc02a23\", \"lda_tuning_results_new.csv\", 454)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 20/20 [1:29:05<00:00, 267.29s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me8SjaWrbtnn",
        "outputId": "2e01de43-9ba9-43bd-8a70-96e4680b8d07"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "        corpus=corpus, id2word=id2word, num_topics=20, random_state=100, \n",
        "        update_every=1, chunksize=100, passes=10, alpha=0.01, eta=0.61, per_word_topics=True\n",
        "        )\n",
        "coherence_model_lda = CoherenceModel(\n",
        "   model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v'\n",
        ")\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score:  0.6787468993133209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DahY1qRSQm0y",
        "outputId": "55694b0c-bd49-4d77-a2ed-a7a12daefc5e"
      },
      "source": [
        "print('Number of unique tokens: %d' % len(id2word)) #dictionary\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique tokens: 19391\n",
            "Number of documents: 11314\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}