{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CE807_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcvK/isAZZb2OKm+5roCx5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ah20776/CE807---Assignment/blob/main/Assignment2/CE807_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7d51qaZQBHN",
        "outputId": "06fae6f1-cde8-4f8e-e1c2-6b0cb076a94a"
      },
      "source": [
        "!pip install pyLDAvis\n",
        "!pip install gensim\n",
        "!pip install spacy"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.2.4)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.15)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.20.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (54.2.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.20.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.20.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfKmTfErQFmj",
        "outputId": "3513234e-aa2e-4a41-9727-bde26e69cc53"
      },
      "source": [
        "# Load news data set\n",
        "# remove meta data headers footers and quotes from news dataset\n",
        "\n",
        "from pprint import pprint\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True,\n",
        "                            random_state=32,\n",
        "                            remove=('headers', 'footers', 'qutes'))\n",
        "\n",
        "#\n",
        "dataset_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=32, remove=('headers', 'footers', 'qutes'))\n",
        "dataset_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=32, remove=('headers', 'footers', 'qutes'))\n",
        "# Check the names of the categories\n",
        "pprint(dataset.target_names)\n",
        "\n",
        "print('data loaded')\n",
        "\n",
        "categories = dataset_train.target_names    # for case categories == None\n",
        "def size_mb(docs):\n",
        "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
        "\n",
        "dataset_train_size_mb = size_mb(dataset_train.data)\n",
        "dataset_test_size_mb = size_mb(dataset_test.data)\n",
        "\n",
        "print(\"%d documents - %0.3fMB (training set)\" % (\n",
        "    len(dataset_train.data), dataset_train_size_mb))\n",
        "print(\"%d documents - %0.3fMB (test set)\" % (\n",
        "    len(dataset_test.data), dataset_test_size_mb))\n",
        "print(\"%d categories\" % len(categories))\n",
        "print()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism',\n",
            " 'comp.graphics',\n",
            " 'comp.os.ms-windows.misc',\n",
            " 'comp.sys.ibm.pc.hardware',\n",
            " 'comp.sys.mac.hardware',\n",
            " 'comp.windows.x',\n",
            " 'misc.forsale',\n",
            " 'rec.autos',\n",
            " 'rec.motorcycles',\n",
            " 'rec.sport.baseball',\n",
            " 'rec.sport.hockey',\n",
            " 'sci.crypt',\n",
            " 'sci.electronics',\n",
            " 'sci.med',\n",
            " 'sci.space',\n",
            " 'soc.religion.christian',\n",
            " 'talk.politics.guns',\n",
            " 'talk.politics.mideast',\n",
            " 'talk.politics.misc',\n",
            " 'talk.religion.misc']\n",
            "data loaded\n",
            "11314 documents - 18.113MB (training set)\n",
            "7532 documents - 11.201MB (test set)\n",
            "20 categories\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnY6QvfEQIOV",
        "outputId": "379014ea-98d7-45e6-ced9-ba57d5850b93"
      },
      "source": [
        "\n",
        "'''\n",
        "Loading Gensim and nltk libraries\n",
        "'''\n",
        "# pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(400)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "from spacy.lang.en.examples import sentences"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLLTGpKcQLCZ"
      },
      "source": [
        "'''\n",
        "Write a function to perform the pre processing steps on the entire dataset\n",
        "'''\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return result"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifEGKJiBQNke",
        "outputId": "652c1853-bcb1-4ab2-ea80-6d889d7643f9"
      },
      "source": [
        "processed_docs = []\n",
        "\n",
        "for doc in dataset_train.data:\n",
        "    processed_docs.append(preprocess(doc))\n",
        "\n",
        "'''\n",
        "Preview 'processed_docs'\n",
        "'''\n",
        "pprint(processed_docs[:2])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['real',\n",
            "  'question',\n",
            "  'opinion',\n",
            "  'motorola',\n",
            "  'processor',\n",
            "  'run',\n",
            "  'compar',\n",
            "  'intel',\n",
            "  'processor',\n",
            "  'run',\n",
            "  'window',\n",
            "  'recal',\n",
            "  'convers',\n",
            "  'run',\n",
            "  'window',\n",
            "  'benchmark',\n",
            "  'speed',\n",
            "  'know',\n",
            "  'true',\n",
            "  'love',\n",
            "  'hear',\n",
            "  'technic',\n",
            "  'data',\n",
            "  'david'],\n",
            " ['current',\n",
            "  'street',\n",
            "  'price',\n",
            "  'follow',\n",
            "  'relev',\n",
            "  'tax',\n",
            "  'simm',\n",
            "  'simm',\n",
            "  'refund',\n",
            "  'possibl',\n",
            "  'export',\n",
            "  'recommend',\n",
            "  'reliabl',\n",
            "  'supplier']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YutYF9dQPf4",
        "outputId": "89ee8d1f-1dd6-47a4-d296-675240920952"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "bigram = gensim.models.Phrases(processed_docs, min_count=5, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[processed_docs], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "def remove_stopwords(texts):\n",
        "   return [[word for word in simple_preprocess(str(doc)) \n",
        "   if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "   return [bigram_mod[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "   [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "   texts_out = []\n",
        "   for sent in texts:\n",
        "      doc = nlp(\" \".join(sent))\n",
        "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "   return texts_out\n",
        "data_words_nostops = remove_stopwords(processed_docs)\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=[\n",
        "   'NOUN', 'ADJ', 'VERB', 'ADV'\n",
        "])\n",
        "print(data_lemmatized[:4]) #it will print the lemmatized data."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['real', 'question', 'opinion', 'window', 'recal', 'conver', 'run', 'window', 'benchmark', 'speed', 'know', 'true', 'love', 'hear', 'technic'], ['current', 'street', 'price', 'follow', 'refund', 'reliabl', 'supplier'], ['help', 'inform', 'card', 'reader', 'recent', 'buy', 'local', 'surplus', 'dealer', 'rear', 'follow', 'inform', 'card', 'reader', 'connector', 'power', 'connector'], ['write', 'sick', 'call', 'legisl', 'unseal', 'involv', 'atroc', 'includ', 'presid', 'attorney_general', 'governor', 'suspend', 'pend', 'serious', 'doubt']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYQWf1z0QSh1",
        "outputId": "79dff8c5-153f-4662-cbb7-1f252819f112"
      },
      "source": [
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "print(corpus[:4]) #it will print the corpus we created above.\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]] \n",
        "#it will print the words with their frequencies.\n",
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "   corpus=corpus, id2word=id2word, num_topics=10, random_state=100, \n",
        "   update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2)], [(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)], [(15, 1), (21, 1), (22, 2), (23, 2), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1)], [(33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgp0x_iTQU_0",
        "outputId": "162602f6-f8c2-4f24-94d6-cd90ce286db6"
      },
      "source": [
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]\n",
        "\n",
        "coherence_model_lda = CoherenceModel(\n",
        "   model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v'\n",
        ")\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.053*\"window\" + 0.045*\"card\" + 0.031*\"driver\" + 0.031*\"problem\" + '\n",
            "  '0.030*\"color\" + 0.023*\"version\" + 0.020*\"email\" + 0.020*\"monitor\" + '\n",
            "  '0.019*\"support\" + 0.019*\"run\"'),\n",
            " (1,\n",
            "  '0.030*\"peopl\" + 0.021*\"right\" + 0.020*\"say\" + 0.016*\"person\" + 0.016*\"true\" '\n",
            "  '+ 0.014*\"fact\" + 0.013*\"reason\" + 0.012*\"argument\" + 0.011*\"live\" + '\n",
            "  '0.011*\"claim\"'),\n",
            " (2,\n",
            "  '0.029*\"sell\" + 0.021*\"model\" + 0.020*\"pain\" + 0.018*\"car\" + 0.014*\"mile\" + '\n",
            "  '0.014*\"price\" + 0.013*\"metal\" + 0.013*\"finger\" + 0.013*\"food\" + '\n",
            "  '0.011*\"clean\"'),\n",
            " (3,\n",
            "  '0.036*\"write\" + 0.032*\"know\" + 0.023*\"think\" + 0.020*\"time\" + 0.016*\"go\" + '\n",
            "  '0.015*\"good\" + 0.014*\"look\" + 0.014*\"come\" + 0.014*\"want\" + 0.012*\"work\"'),\n",
            " (4,\n",
            "  '0.034*\"govern\" + 0.025*\"kill\" + 0.021*\"exist\" + 0.019*\"object\" + '\n",
            "  '0.016*\"deal\" + 0.015*\"attack\" + 0.014*\"moral\" + 0.013*\"armenian\" + '\n",
            "  '0.013*\"peopl\" + 0.011*\"state\"'),\n",
            " (5,\n",
            "  '0.053*\"game\" + 0.030*\"team\" + 0.030*\"play\" + 0.026*\"year\" + 0.025*\"player\" '\n",
            "  '+ 0.016*\"season\" + 0.016*\"point\" + 0.015*\"night\" + 0.014*\"panel\" + '\n",
            "  '0.014*\"lose\"'),\n",
            " (6,\n",
            "  '0.022*\"event\" + 0.019*\"report\" + 0.017*\"univer\" + 0.014*\"nation\" + '\n",
            "  '0.014*\"year\" + 0.012*\"research\" + 0.012*\"suffer\" + 0.012*\"blood\" + '\n",
            "  '0.011*\"doctor\" + 0.011*\"treatment\"'),\n",
            " (7,\n",
            "  '0.019*\"high\" + 0.018*\"list\" + 0.017*\"send\" + 0.016*\"mail\" + 0.015*\"number\" '\n",
            "  '+ 0.013*\"inform\" + 0.013*\"current\" + 0.012*\"develop\" + 0.011*\"cost\" + '\n",
            "  '0.011*\"space\"'),\n",
            " (8,\n",
            "  '0.034*\"output\" + 0.026*\"input\" + 0.023*\"manual\" + 0.020*\"print\" + '\n",
            "  '0.020*\"site\" + 0.017*\"recommend\" + 0.017*\"shape\" + 0.017*\"printer\" + '\n",
            "  '0.017*\"page\" + 0.016*\"flight\"'),\n",
            " (9,\n",
            "  '0.045*\"drive\" + 0.045*\"file\" + 0.037*\"wire\" + 0.036*\"program\" + '\n",
            "  '0.024*\"control\" + 0.017*\"disk\" + 0.017*\"type\" + 0.017*\"speed\" + '\n",
            "  '0.012*\"display\" + 0.012*\"chip\"')]\n",
            "\n",
            "Coherence Score:  0.4487095169430484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDklpYUcLu9R"
      },
      "source": [
        "#to find the best hyperparameters\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        "# Topics range\n",
        "min_topics = 2\n",
        "max_topics = 32\n",
        "step_size = 2\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "    \n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "                    \n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
        "    files.download('lda_tuning_results.csv')\n",
        "    pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EJKcYF0_QXL-",
        "outputId": "1e0ca38e-45d6-4b65-b796-07ce5398d81c"
      },
      "source": [
        "#finding the best number of topics with best hyperparameters\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "#With best parameters\n",
        "\n",
        "# Topics range\n",
        "min_topics = 5\n",
        "max_topics = 105\n",
        "step_size = 5\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "\n",
        "model_results = {'Topics': [],'Coherence': []}\n",
        "\n",
        "\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=len(topics_range))\n",
        "    for k in topics_range:\n",
        "        lda_model = gensim.models.ldamodel.LdaModel(\n",
        "        corpus=corpus, id2word=id2word, num_topics=k, random_state=100, \n",
        "        update_every=1, chunksize=100, passes=10, alpha=0.01, eta=0.61, per_word_topics=True\n",
        "        )\n",
        "\n",
        "    #print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
        "\n",
        "        coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "        coherence_lda = coherence_model_lda.get_coherence()\n",
        "        model_results['Topics'].append(k)\n",
        "        model_results['Coherence'].append(coherence_lda)\n",
        "        #print('\\nCoherence Score: ', coherence_lda)\n",
        "        pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('lda_tuning_results_new.csv', index=False)\n",
        "    files.download('lda_tuning_results_new.csv')\n",
        "    pbar.close()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [1:29:05<00:00, 413.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0d671a8e-db6c-4c5e-a3b5-6d0c7cc02a23\", \"lda_tuning_results_new.csv\", 454)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 20/20 [1:29:05<00:00, 267.29s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me8SjaWrbtnn",
        "outputId": "cbd39013-3705-4316-c0b1-94e0edf1226e"
      },
      "source": [
        "lda_model_best = gensim.models.ldamodel.LdaModel(\n",
        "        corpus=corpus, id2word=id2word, num_topics=50, random_state=100, \n",
        "        update_every=1, chunksize=100, passes=10, alpha=0.01, eta=0.61, per_word_topics=True\n",
        "        )\n",
        "coherence_model_lda_best = CoherenceModel(\n",
        "   model=lda_model_best, texts=data_lemmatized, dictionary=id2word, coherence='c_v'\n",
        ")\n",
        "coherence_lda_best = coherence_model_lda_best.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_best)\n",
        "\n",
        "print('\\nPerplexity: ', lda_model_best.log_perplexity(corpus))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score:  0.8125950391134878\n",
            "\n",
            "Perplexity:  -7.742516949092922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DahY1qRSQm0y",
        "outputId": "55694b0c-bd49-4d77-a2ed-a7a12daefc5e"
      },
      "source": [
        "print('Number of unique tokens: %d' % len(id2word)) #dictionary\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique tokens: 19391\n",
            "Number of documents: 11314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqabRpHKXVYn",
        "outputId": "cc512e70-2050-4f0c-8cc9-951cb34e856f"
      },
      "source": [
        "#working with test data\n",
        "\n",
        "processed_docs_test = []\n",
        "\n",
        "for doc in dataset_test.data:\n",
        "    processed_docs_test.append(preprocess(doc))\n",
        "\n",
        "'''\n",
        "Preview 'processed_docs'\n",
        "'''\n",
        "pprint(processed_docs_test[:2])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['radio',\n",
            "  'shack',\n",
            "  'store',\n",
            "  'sell',\n",
            "  'state',\n",
            "  'sure',\n",
            "  'intertan',\n",
            "  'carri',\n",
            "  'australia',\n",
            "  'cost',\n",
            "  'charg',\n",
            "  'fluoresc',\n",
            "  'glow',\n",
            "  'expos'],\n",
            " ['observ',\n",
            "  'behaviour',\n",
            "  'cure',\n",
            "  'workstat',\n",
            "  'network',\n",
            "  'go',\n",
            "  'deaf',\n",
            "  'form',\n",
            "  'communic',\n",
            "  'workstat',\n",
            "  'go',\n",
            "  'network',\n",
            "  'workstaton',\n",
            "  'interact',\n",
            "  'time',\n",
            "  'later',\n",
            "  'go',\n",
            "  'deaf']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdSIwmfyXm-0",
        "outputId": "e8e90147-56dd-4672-c98e-ab7114a09982"
      },
      "source": [
        "bigram_test = gensim.models.Phrases(processed_docs_test, min_count=5, threshold=100)\n",
        "trigram_test = gensim.models.Phrases(bigram_test[processed_docs_test], threshold=100)\n",
        "bigram_mod_test = gensim.models.phrases.Phraser(bigram_test)\n",
        "trigram_mod_test = gensim.models.phrases.Phraser(trigram_test)\n",
        "def remove_stopwords(texts):\n",
        "   return [[word for word in simple_preprocess(str(doc)) \n",
        "   if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "   return [bigram_mod_test[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "   [trigram_mod_test[bigram_mod_test[doc]] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "   texts_out_test = []\n",
        "   for sent in texts:\n",
        "      doc = nlp(\" \".join(sent))\n",
        "      texts_out_test.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "   return texts_out_test\n",
        "data_words_nostops_test = remove_stopwords(processed_docs_test)\n",
        "data_words_bigrams_test = make_bigrams(data_words_nostops_test)\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "data_lemmatized_test = lemmatization(data_words_bigrams_test, allowed_postags=[\n",
        "   'NOUN', 'ADJ', 'VERB', 'ADV'\n",
        "])\n",
        "print(data_lemmatized_test[:4]) #it will print the lemmatized data."
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['store', 'sell', 'state', 'sure'], ['observ', 'behaviour', 'cure', 'workstat', 'network', 'go', 'deaf', 'form', 'communic', 'workstat', 'go', 'time', 'later', 'go', 'deaf'], ['write', 'obvious', 'equal', 'mother', 'mean', 'close', 'identif', 'redempt', 'work', 'assum', 'soul', 'perfect', 'obvious', 'superior', 'glorif', 'previous', 'mention', 'say', 'answer', 'logic', 'protestant', 'odium', 'religion', 'allow', 'represent', 'woman', 'protestant', 'obvious', 'give', 'attent', 'sign', 'time', 'point', 'equal', 'woman', 'equal', 'woman', 'bride', 'person', 'demand', 'equal', 'person', 'represent', 'dogmat', 'accord', 'view', 'mistress', 'function', 'rate', 'need', 'get', 'theolog', 'human', 'psych', 'actual', 'exist', 'natur'], ['hold', 'opinion', 'write', 'letter', 'represent', 'view', 'group', 'belong', 'view', 'believ', 'major', 'poster', 'hockey', 'share', 'view', 'intent', 'allow', 'simpli', 'ahead', 'make', 'feel', 'clear', 'thank', 'speak', 'feel', 'import', 'letter', 'represent', 'view', 'group', 'repre', 'subset', 'belong', 'large', 'group', 'group', 'look', 'way', 'state', 'oppos', 'viewpoint', 'come', 'mind', 'follow', 'subt', 'result', 'clear', 'direct', 'polit', 'result', 'probabl', 'spark', 'interest', 'conver', 'direct', 'result', 'valid', 'go', 'right', 'window', 'agre', 'say', 'certain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCeRgbj9ZFQL",
        "outputId": "2247f5eb-c512-42e1-f293-ef7b22a84ed6"
      },
      "source": [
        "id2word_test = corpora.Dictionary(data_lemmatized_test)\n",
        "texts_test = data_lemmatized_test\n",
        "corpus_test = [id2word_test.doc2bow(text) for text in texts_test]\n",
        "print(corpus_test[:4]) #it will print the corpus we created above.\n",
        "[[(id2word_test[id], freq) for id, freq in cp] for cp in corpus_test[:4]] \n",
        "#it will print the words with their frequencies.\n",
        "\n",
        "lda_model_test = gensim.models.ldamodel.LdaModel(\n",
        "        corpus=corpus_test, id2word=id2word_test, num_topics=50, random_state=100, \n",
        "        update_every=1, chunksize=100, passes=10, alpha=0.01, eta=0.61, per_word_topics=True\n",
        "        )\n",
        "coherence_model_lda_test = CoherenceModel(\n",
        "   model=lda_model_test, texts=data_lemmatized_test, dictionary=id2word_test, coherence='c_v'\n",
        ")\n",
        "coherence_lda_test = coherence_model_lda_test.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_test)\n",
        "\n",
        "print('\\nPerplexity: ', lda_model_test.log_perplexity(corpus_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 3), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2)], [(13, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 4), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 3), (41, 1), (42, 1), (43, 2), (44, 1), (45, 1), (46, 2), (47, 1), (48, 1), (49, 1), (50, 1), (51, 2), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 3), (59, 1), (60, 1)], [(1, 1), (9, 1), (17, 1), (51, 2), (52, 1), (57, 4), (60, 1), (61, 1), (62, 1), (63, 1), (64, 2), (65, 1), (66, 2), (67, 1), (68, 1), (69, 2), (70, 2), (71, 1), (72, 4), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 3), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1)]]\n",
            "\n",
            "Coherence Score:  0.5196518971058679\n",
            "\n",
            "Perplexity:  -7.751564791965303\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}